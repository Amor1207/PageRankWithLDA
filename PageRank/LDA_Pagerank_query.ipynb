{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-31T01:27:44.303695Z",
     "start_time": "2024-07-31T01:27:43.341376Z"
    }
   },
   "source": [
    "import jieba\n",
    "from docx import Document\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:27:45.240506Z",
     "start_time": "2024-07-31T01:27:45.237505Z"
    }
   },
   "cell_type": "code",
   "source": "### 分词",
   "id": "676e56d5f085dad9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:27:53.189571Z",
     "start_time": "2024-07-31T01:27:53.179126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 读取本地停用词表\n",
    "stopwords = set(open('C:/pythonProject/train_model/stopwords.txt', 'r', encoding='utf-8').read().splitlines())"
   ],
   "id": "2fdeef49fef98b3b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:27:54.398676Z",
     "start_time": "2024-07-31T01:27:54.384170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 下载停用词表\n",
    "# url = \"https://raw.githubusercontent.com/goto456/stopwords/master/cn_stopwords.txt\"\n",
    "# stopwords = requests.get(url).text\n",
    "# stopwords = set(stopwords.splitlines())"
   ],
   "id": "882267c4fff7daa6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:27:55.277946Z",
     "start_time": "2024-07-31T01:27:55.257512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 读取文本文件\n",
    "def read_text_from_docx(file_path):\n",
    "    document = Document(file_path)\n",
    "    text = []\n",
    "    \n",
    "    for para in document.paragraphs:\n",
    "        text.append(para.text)\n",
    "    \n",
    "    return \"\\n\".join(text)"
   ],
   "id": "398d488268b4b99f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:27:57.662573Z",
     "start_time": "2024-07-31T01:27:57.652635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 读取停用词\n",
    "def preprocess_chinese_text(text):\n",
    "    # 使用jieba进行分词和词性标注\n",
    "    words = jieba.cut_for_search(text, HMM=True)\n",
    "    \n",
    "    # 过滤停用词和进行词性选择\n",
    "    filtered_words = [word for word in words if word not in stopwords and word.strip() and not word.isspace()]  \n",
    "\n",
    "    return filtered_words"
   ],
   "id": "bc07e15f220634f3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:28:11.037710Z",
     "start_time": "2024-07-31T01:28:11.028766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_documents(directory, output_filename):\n",
    "    \"\"\"处理目录下的所有.docx文件，并保存分词结果到CSV文件中。\"\"\"\n",
    "    data = []\n",
    "    documents_name = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".docx\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            text = read_text_from_docx(file_path)\n",
    "            segmented_text = preprocess_chinese_text(text)\n",
    "            data.append({'filename': filename, 'text': segmented_text})\n",
    "            documents_name.append(filename)\n",
    "    \n",
    "    # 将数据转换成DataFrame并保存到CSV文件\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "    \n",
    "    return documents_name"
   ],
   "id": "38c14355acedd8d1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:28:25.412585Z",
     "start_time": "2024-07-31T01:28:25.405860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 指定包含.docx文件的目录和输出CSV文件的路径\n",
    "directory = \"技术1\"\n",
    "output_filename = \"segmented_text.csv\""
   ],
   "id": "7a0956d0a43ca53b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:28:26.860380Z",
     "start_time": "2024-07-31T01:28:26.124255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 处理指定目录中的所有.docx文件并将结果保存为CSV\n",
    "document_names = process_documents(directory, output_filename)"
   ],
   "id": "3ddee15238c1d668",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\LIUYIN~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.448 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:28:29.129639Z",
     "start_time": "2024-07-31T01:28:29.125639Z"
    }
   },
   "cell_type": "code",
   "source": "### LDA建模",
   "id": "5280a7baccc75f3d",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:28:34.879665Z",
     "start_time": "2024-07-31T01:28:33.898055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gensim import corpora, models\n",
    "from gensim.models.ldamodel import LdaModel"
   ],
   "id": "45ca92a409efb25e",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:28:35.663069Z",
     "start_time": "2024-07-31T01:28:35.652580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"从CSV文件加载数据。\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    # 假设CSV文件中每行包含一个文档的分词文本\n",
    "    # 将字符串形式的列表转换为真正的列表\n",
    "    df['text'] = df['text'].apply(eval)\n",
    "    return df['text'].tolist()"
   ],
   "id": "bec02d29b07762f9",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:28:36.664202Z",
     "start_time": "2024-07-31T01:28:36.660086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_corpus(documents):\n",
    "    \"\"\"准备语料库和词典，用于LDA模型。\"\"\"\n",
    "    # 创建字典\n",
    "    dictionary = corpora.Dictionary(documents)\n",
    "    # 使用字典转换文本数据为词袋模型\n",
    "    corpus = [dictionary.doc2bow(text) for text in documents]\n",
    "    return dictionary, corpus"
   ],
   "id": "1192f86599791ee",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:28:47.421296Z",
     "start_time": "2024-07-31T01:28:47.411251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def lda_model(corpus, dictionary, num_topics=5):\n",
    "    \"\"\"训练LDA模型并返回。\"\"\"\n",
    "    # 设置训练LDA模型的参数\n",
    "    lda = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=30, random_state=42)\n",
    "    return lda\n"
   ],
   "id": "c5dcaafe4e52f942",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:28:48.877876Z",
     "start_time": "2024-07-31T01:28:48.866964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载数据\n",
    "file_path = 'segmented_text.csv'\n",
    "documents = load_data(file_path)"
   ],
   "id": "45af8cd334e2ae65",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:28:50.295948Z",
     "start_time": "2024-07-31T01:28:50.288603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 准备语料库和字典\n",
    "dictionary, corpus = prepare_corpus(documents)"
   ],
   "id": "3fd9af8def7b96ad",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:28:51.137081Z",
     "start_time": "2024-07-31T01:28:51.124480Z"
    }
   },
   "cell_type": "code",
   "source": "### 评价LDA模型 ###",
   "id": "6146048b38237d2c",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:28:55.948858Z",
     "start_time": "2024-07-31T01:28:55.935522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#计算coherence score\n",
    "def coherence(num_topics, dictionary, corpus, documents):\n",
    "    lda = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=30, random_state=42)\n",
    "    coherence_model_lda = models.CoherenceModel(model=lda, texts=documents, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    return coherence_lda\n"
   ],
   "id": "8fc81d5639db8307",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:28:58.379282Z",
     "start_time": "2024-07-31T01:28:58.372977Z"
    }
   },
   "cell_type": "code",
   "source": "### 储存主题分布 ###",
   "id": "a2d4ffa97638191e",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:29:05.611521Z",
     "start_time": "2024-07-31T01:29:05.506817Z"
    }
   },
   "cell_type": "code",
   "source": "lda = LdaModel.load(\"C:/pythonProject/train_model/lda_model_35\")",
   "id": "efd5f70078279cf1",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:32:01.743623Z",
     "start_time": "2024-07-31T01:32:01.710609Z"
    }
   },
   "cell_type": "code",
   "source": "lda.show_topics()",
   "id": "6485a9df761e9cd1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8,\n",
       "  '0.099*\"姚明\" + 0.032*\"火箭\" + 0.020*\"年\" + 0.018*\"退役\" + 0.018*\"篮球\" + 0.018*\"NBA\" + 0.011*\"生涯\" + 0.011*\"中国\" + 0.011*\"火箭队\" + 0.010*\"职业\"'),\n",
       " (26,\n",
       "  '0.022*\"中国\" + 0.021*\"裁判\" + 0.016*\"姚明\" + 0.012*\"上海\" + 0.012*\"篮协\" + 0.012*\"CBA\" + 0.011*\"篮球\" + 0.009*\"男篮\" + 0.008*\"布里\" + 0.007*\"邓华德\"'),\n",
       " (33,\n",
       "  '0.018*\"足球\" + 0.013*\"教练\" + 0.012*\"中国\" + 0.009*\"年\" + 0.008*\"说\" + 0.006*\"球员\" + 0.006*\"俱乐部\" + 0.005*\"孩子\" + 0.005*\"国家\" + 0.004*\"岁\"'),\n",
       " (3,\n",
       "  '0.038*\"奇才\" + 0.017*\"阿联\" + 0.016*\"沃尔\" + 0.015*\"易建联\" + 0.013*\"比赛\" + 0.011*\"中\" + 0.010*\"赛季\" + 0.010*\"格里\" + 0.009*\"格里芬\" + 0.008*\"步行\"'),\n",
       " (14,\n",
       "  '0.030*\"说\" + 0.020*\"记者\" + 0.016*\"比赛\" + 0.016*\"球迷\" + 0.008*\"时\" + 0.008*\"训练\" + 0.007*\"做\" + 0.007*\"想\" + 0.006*\"采访\" + 0.006*\"时间\"'),\n",
       " (17,\n",
       "  '0.039*\"大利\" + 0.036*\"意大利\" + 0.015*\"里\" + 0.014*\"皮\" + 0.010*\"世界\" + 0.008*\"意大利队\" + 0.008*\"新西兰\" + 0.008*\"西兰\" + 0.007*\"世界杯\" + 0.007*\"比赛\"'),\n",
       " (22,\n",
       "  '0.056*\"法国\" + 0.031*\"法国队\" + 0.014*\"世界\" + 0.013*\"梅内\" + 0.013*\"内克\" + 0.013*\"梅内克\" + 0.012*\"世界杯\" + 0.009*\"内尔\" + 0.008*\"阿内\" + 0.008*\"阿内尔卡\"'),\n",
       " (18,\n",
       "  '0.015*\"湖人\" + 0.014*\"科比\" + 0.012*\"小牛\" + 0.011*\"赛季\" + 0.010*\"中\" + 0.009*\"球队\" + 0.008*\"湖人队\" + 0.007*\"索尔\" + 0.007*\"加索尔\" + 0.007*\"比赛\"'),\n",
       " (30,\n",
       "  '0.060*\"教练\" + 0.021*\"主教\" + 0.021*\"主教练\" + 0.017*\"主帅\" + 0.012*\"执教\" + 0.011*\"竞聘\" + 0.009*\"足协\" + 0.008*\"工作\" + 0.008*\"殷铁生\" + 0.006*\"刘春明\"'),\n",
       " (24,\n",
       "  '0.025*\"世界\" + 0.022*\"世界杯\" + 0.017*\"德国\" + 0.015*\"比赛\" + 0.013*\"西班牙\" + 0.009*\"利亚\" + 0.009*\"球队\" + 0.008*\"中\" + 0.008*\"巴西\" + 0.007*\"年\"')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:29:07.792098Z",
     "start_time": "2024-07-31T01:29:07.776211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 获取每个文档的主题分布\n",
    "def get_topic_distribution(lda, corpus):\n",
    "    topic_distribution = []\n",
    "    for doc in corpus:\n",
    "        topic_distribution.append(lda.get_document_topics(doc,minimum_probability=0.0))\n",
    "    return topic_distribution"
   ],
   "id": "50474be72af8127c",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:34:39.979451Z",
     "start_time": "2024-07-31T01:34:39.972913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 保存主题分布到文件\n",
    "def save_topic_distribution(lda, corpus, output_filename):\n",
    "    topic_distribution = get_topic_distribution(lda, corpus)\n",
    "    df = pd.DataFrame(topic_distribution)\n",
    "    df.to_csv(output_filename, index=False)"
   ],
   "id": "1c2bc4305d813d46",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:34:41.075552Z",
     "start_time": "2024-07-31T01:34:41.053663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_filename = 'topic_distribution.csv'\n",
    "save_topic_distribution(lda, corpus, output_filename)"
   ],
   "id": "adba5c35a21f6f61",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T02:38:28.479887Z",
     "start_time": "2024-07-31T02:38:28.417814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 主题可视化 ###\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "data = pyLDAvis.gensim.prepare(lda, corpus, dictionary)\n",
    "pyLDAvis.save_html(data, 'lda.html')"
   ],
   "id": "5a92879ba081342c",
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "\n * Not all rows (distributions) in topic_term_dists sum to 1.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValidationError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[53], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpyLDAvis\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgensim\u001B[39;00m\n\u001B[0;32m      4\u001B[0m pyLDAvis\u001B[38;5;241m.\u001B[39menable_notebook()\n\u001B[1;32m----> 5\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mpyLDAvis\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgensim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprepare\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlda\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcorpus\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdictionary\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m pyLDAvis\u001B[38;5;241m.\u001B[39msave_html(data, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlda.html\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyLDAvis\\gensim.py:123\u001B[0m, in \u001B[0;36mprepare\u001B[1;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001B[0m\n\u001B[0;32m     78\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Transforms the Gensim TopicModel and related corpus and dictionary into\u001B[39;00m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;124;03mthe data structures needed for the visualization.\u001B[39;00m\n\u001B[0;32m     80\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;124;03mSee `pyLDAvis.prepare` for **kwargs.\u001B[39;00m\n\u001B[0;32m    121\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    122\u001B[0m opts \u001B[38;5;241m=\u001B[39m fp\u001B[38;5;241m.\u001B[39mmerge(_extract_data(topic_model, corpus, dictionary, doc_topic_dist), kwargs)\n\u001B[1;32m--> 123\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m vis_prepare(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mopts)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyLDAvis\\_prepare.py:408\u001B[0m, in \u001B[0;36mprepare\u001B[1;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics, start_index)\u001B[0m\n\u001B[0;32m    406\u001B[0m doc_lengths \u001B[38;5;241m=\u001B[39m _series_with_name(doc_lengths, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdoc_length\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    407\u001B[0m vocab \u001B[38;5;241m=\u001B[39m _series_with_name(vocab, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvocab\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 408\u001B[0m \u001B[43m_input_validate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtopic_term_dists\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdoc_topic_dists\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdoc_lengths\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mterm_frequency\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    409\u001B[0m R \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(R, \u001B[38;5;28mlen\u001B[39m(vocab))\n\u001B[0;32m    411\u001B[0m topic_freq \u001B[38;5;241m=\u001B[39m doc_topic_dists\u001B[38;5;241m.\u001B[39mmul(doc_lengths, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msum()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyLDAvis\\_prepare.py:69\u001B[0m, in \u001B[0;36m_input_validate\u001B[1;34m(*args)\u001B[0m\n\u001B[0;32m     67\u001B[0m res \u001B[38;5;241m=\u001B[39m _input_check(\u001B[38;5;241m*\u001B[39margs)\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m res:\n\u001B[1;32m---> 69\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ValidationError(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m * \u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m s \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m res]))\n",
      "\u001B[1;31mValidationError\u001B[0m: \n * Not all rows (distributions) in topic_term_dists sum to 1."
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:29:10.211989Z",
     "start_time": "2024-07-31T01:29:10.198589Z"
    }
   },
   "cell_type": "code",
   "source": "### 计算文档之间的主题距离 ###",
   "id": "d69c80313f25ecd9",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:29:10.766996Z",
     "start_time": "2024-07-31T01:29:10.756535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 计算两个两个主题分布之间的曼哈顿距离\n",
    "def calculate_topic_distance_abs_diff(doc_topics_i, doc_topics_j):\n",
    "    # 保证每个主题的概率被考虑到，即使某些主题在某文档中的概率为0\n",
    "    # 将主题分布转换为字典形式\n",
    "    topic_dist_i = dict(doc_topics_i)\n",
    "    topic_dist_j = dict(doc_topics_j)\n",
    "    \n",
    "    # 获取所有主题的并集\n",
    "    all_topics = set(topic_dist_i.keys()).union(set(topic_dist_j.keys()))\n",
    "    distance = sum(abs(topic_dist_i.get(topic, 0) - topic_dist_j.get(topic, 0)) for topic in all_topics)\n",
    "    return distance"
   ],
   "id": "eb247de34b67e446",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:29:11.466765Z",
     "start_time": "2024-07-31T01:29:11.450614Z"
    }
   },
   "cell_type": "code",
   "source": "doc_topics = get_topic_distribution(lda, corpus)",
   "id": "943fe29db978b10d",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:29:11.970263Z",
     "start_time": "2024-07-31T01:29:11.957299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 计算所有文档之间的主题距离 \n",
    "def calculate_all_topic_distances(doc_topics):\n",
    "    num_docs = len(doc_topics)\n",
    "    distances = [[0] * num_docs for _ in range(num_docs)]\n",
    "    for i in range(num_docs):\n",
    "        for j in range(i+1, num_docs):\n",
    "            distance = calculate_topic_distance_abs_diff(doc_topics[i], doc_topics[j])\n",
    "            distances[i][j] = distances[j][i] = distance\n",
    "    return distances"
   ],
   "id": "7d092f00b6218596",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:29:12.800791Z",
     "start_time": "2024-07-31T01:29:12.787149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 处理查询并计算查询的主题分布 \n",
    "def preprocess_query(query, lda, dictionary):\n",
    "    query_bow = dictionary.doc2bow(jieba.cut_for_search(query, HMM=True))\n",
    "    query_topics = lda.get_document_topics(query_bow, minimum_probability=0.0)\n",
    "    return dict(query_topics)\n",
    "\n",
    "query = '技术'\n",
    "query_topics = preprocess_query(query, lda, dictionary)"
   ],
   "id": "ad789f3e83d456eb",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:29:13.595792Z",
     "start_time": "2024-07-31T01:29:13.581335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 将主题距离转换为相似度\n",
    "# 阈值threshold用于确定两个文档之间是否存在链接，按需调整\n",
    "def convert_distance_to_similarity(distances, threshold=0.1):\n",
    "    max_distance = max(max(row) for row in distances if row)\n",
    "    similarity_matrix = []\n",
    "    links = []\n",
    "    for i, row in enumerate(distances):\n",
    "        new_row = []\n",
    "        link_row = []\n",
    "        for j, dist in enumerate(row):\n",
    "            similarity = 1 - (dist / max_distance) if max_distance else 1\n",
    "            new_row.append(similarity)\n",
    "            if similarity > threshold and i != j:\n",
    "                link_row.append(j)\n",
    "        similarity_matrix.append(new_row)\n",
    "        links.append(link_row)\n",
    "    return similarity_matrix, links"
   ],
   "id": "14f779654a0f0469",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:29:15.293208Z",
     "start_time": "2024-07-31T01:29:15.282224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_query_similarity(query_topics, doc_topics):\n",
    "    num_docs = len(doc_topics)\n",
    "    distances = [[0] for _ in range(num_docs)]\n",
    "    for i in range(num_docs):\n",
    "        distance = calculate_topic_distance_abs_diff(query_topics, doc_topics[i])\n",
    "        distances[i] = distance\n",
    "    max_distance = max(distances) if distances else 0\n",
    "    query_similarity = [1 - (dist / max_distance) if max_distance else 1 for dist in distances]\n",
    "    return query_similarity"
   ],
   "id": "d0423f15e89b7726",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:29:16.296687Z",
     "start_time": "2024-07-31T01:29:16.284260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def adjust_link_weights(doc_topics, query_similarity, links, base_weight=0.01):\n",
    "    adjusted_weights = []\n",
    "    for i, topics in enumerate(doc_topics):\n",
    "        doc_similarity = query_similarity[i]\n",
    "        # 将基础权重添加到与查询相关的文档上\n",
    "        row_weights = [base_weight + doc_similarity if j in links[i] else 0 for j in range(len(doc_topics))]\n",
    "        adjusted_weights.append(row_weights)\n",
    "    return adjusted_weights\n"
   ],
   "id": "79eadadd650b8597",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:29:17.428662Z",
     "start_time": "2024-07-31T01:29:17.411292Z"
    }
   },
   "cell_type": "code",
   "source": "query_similarity = calculate_query_similarity(query_topics, doc_topics)",
   "id": "6ef615cea55f7146",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:29:18.461212Z",
     "start_time": "2024-07-31T01:29:18.443840Z"
    }
   },
   "cell_type": "code",
   "source": "query_similarity",
   "id": "cf9cf17dd170669b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4601869705915761,\n",
       " 0.24386147825803794,\n",
       " 0.15967412767933864,\n",
       " 0.01820093688090152,\n",
       " 0.0002984764617403046,\n",
       " 0.08964853190209754,\n",
       " 0.3495462898577584,\n",
       " 0.025152324235296897,\n",
       " 0.14351344073932715,\n",
       " 0.0,\n",
       " 0.20233717625530168,\n",
       " 0.12719021067869796,\n",
       " 0.24241894778584272,\n",
       " 0.2388192418147178,\n",
       " 0.16795632991711684,\n",
       " 0.24895084149501012,\n",
       " 0.020854069671643827,\n",
       " 0.03811001773482059,\n",
       " 0.11505013461046687,\n",
       " 0.014691897979240531]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:29:20.510618Z",
     "start_time": "2024-07-31T01:29:20.499057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 计算PageRank\n",
    "def page_rank(links, similarities, adjusted_weights, alpha=0.85, convergence_threshold=0.0001):\n",
    "    N = len(links)\n",
    "    pr = np.ones(N) / N  # 初始化PR值，总和为1\n",
    "    # 进行迭代直到收敛\n",
    "    while True:\n",
    "        new_pr = np.zeros(N)\n",
    "        for i in range(N):\n",
    "            link_contributions = 0\n",
    "            for j in links[i]:  # 遍历节点i的所有出链节点j\n",
    "                if len(links[j]) > 0:  # 避免除以零\n",
    "                    # 结合 adjusted_weights 和 similarities 作为权重\n",
    "                    link_weight = adjusted_weights[j][i] * similarities[j][i]\n",
    "                    link_contributions += pr[j] * link_weight / len(links[j])\n",
    "            new_pr[i] = (1 - alpha) / N + alpha * link_contributions\n",
    "        # 归一化新的PageRank值，确保它们的总和为1\n",
    "        new_pr /= np.sum(new_pr)  # 归一化步骤\n",
    "        \n",
    "        change = np.linalg.norm(new_pr - pr)\n",
    "        if change < convergence_threshold:\n",
    "            break\n",
    "        pr = new_pr\n",
    "    return pr"
   ],
   "id": "5a87cd87d4bb5f8",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:29:21.378912Z",
     "start_time": "2024-07-31T01:29:21.364442Z"
    }
   },
   "cell_type": "code",
   "source": "distances = calculate_all_topic_distances(doc_topics)",
   "id": "629f59ce5e7f2afd",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:29:21.862946Z",
     "start_time": "2024-07-31T01:29:21.853099Z"
    }
   },
   "cell_type": "code",
   "source": "similarities, links = convert_distance_to_similarity(distances)",
   "id": "84c9a5f222b8b18f",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:29:22.260333Z",
     "start_time": "2024-07-31T01:29:22.250207Z"
    }
   },
   "cell_type": "code",
   "source": "adjusted_weights = adjust_link_weights(doc_topics, query_similarity, links)",
   "id": "b16a7b9ec948dd79",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:29:22.902273Z",
     "start_time": "2024-07-31T01:29:22.897272Z"
    }
   },
   "cell_type": "code",
   "source": "pr = page_rank(links, similarities, adjusted_weights)",
   "id": "91e1b56573085687",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:29:23.895950Z",
     "start_time": "2024-07-31T01:29:23.885933Z"
    }
   },
   "cell_type": "code",
   "source": "pr",
   "id": "ed4810d01775c338",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05211678, 0.05388888, 0.05242332, 0.04033342, 0.05289948,\n",
       "       0.05356109, 0.05318298, 0.05209765, 0.05222325, 0.05060618,\n",
       "       0.05266906, 0.05009194, 0.05096044, 0.05037129, 0.04907672,\n",
       "       0.04955003, 0.05007369, 0.03965215, 0.05395474, 0.04026691])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:29:28.981361Z",
     "start_time": "2024-07-31T01:29:28.966634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 按PageRank值对文档进行排序\n",
    "pagerank_score = pr\n",
    "doc_pagerank = list(zip(document_names, pagerank_score))\n",
    "\n",
    "# 按PageRank分数降序排序\n",
    "sorted_doc_pagerank = sorted(doc_pagerank, key=lambda x: x[1], reverse=True)\n"
   ],
   "id": "14495b4cf725b139",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T01:29:29.973640Z",
     "start_time": "2024-07-31T01:29:29.969628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 打印排序后的结果\n",
    "for doc_name, pr_score in sorted_doc_pagerank:\n",
    "    print(f\"{doc_name}: {pr_score}\")"
   ],
   "id": "4c7e5f65884a6e7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19_技术与环境的可持续发展.docx: 0.05395474019419387\n",
      "02_现代技术的发展.docx: 0.05388887738210079\n",
      "06_技术和社会变革.docx: 0.0535610878609115\n",
      "07_技术与环境保护.docx: 0.053182978704172865\n",
      "05_技术进步对经济的推动.docx: 0.05289948463443641\n",
      "11_技术在航空业的革新.docx: 0.05266906448112079\n",
      "03_技术与教育.docx: 0.052423319227500834\n",
      "09_智能技术的未来.docx: 0.05222324828599845\n",
      "01_技术革命.docx: 0.052116780209683654\n",
      "08_技术在农业中的应用.docx: 0.052097648914056535\n",
      "13_数字技术与数据安全.docx: 0.05096043526383481\n",
      "10_技术在交通领域的变革.docx: 0.05060618368848751\n",
      "14_技术在金融服务中的应用.docx: 0.050371290933558105\n",
      "12_技术推动的社会变化.docx: 0.05009194281439715\n",
      "17_技术与文化的互动.docx: 0.05007369156372026\n",
      "16_技术对教育的长远影响.docx: 0.049550030748569014\n",
      "15_技术在建筑行业的应用.docx: 0.04907672411951765\n",
      "04_技术在医疗中的应用.docx: 0.04033341882045411\n",
      "20_技术在全球治理中的作用.docx: 0.040266906370827325\n",
      "18_未来技术的道德挑战.docx: 0.039652145782458355\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T02:55:29.442950Z",
     "start_time": "2024-07-31T02:55:29.427946Z"
    }
   },
   "cell_type": "code",
   "source": "### 结合点击率（CTR）分析 ###",
   "id": "2c5a582dadecbf1a",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T02:55:30.038775Z",
     "start_time": "2024-07-31T02:55:30.021770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 假设 clicks 是上传的点击次数数组\n",
    "clicks = np.array([5, 10, 15, 20, 25, 5, 10, 15, 20, 25, 5, 10, 15, 20, 25, 5, 10, 15, 20, 25])"
   ],
   "id": "4214f6ae1d20c19a",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T02:55:30.427222Z",
     "start_time": "2024-07-31T02:55:30.418218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 将点击次数转换为点击率（CTR）\n",
    "# 这里假设每个文档都展示了相同的次数，CTR = 点击次数 / 最大点击次数\n",
    "max_clicks = np.max(clicks)\n",
    "ctr = clicks / max_clicks  # 归一化点击次数作为CTR的简化模型"
   ],
   "id": "3e636c04b12d01c5",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T02:55:30.659593Z",
     "start_time": "2024-07-31T02:55:30.642600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 结合PageRank和CTR计算最终排序\n",
    "def page_rank_with_ctr(links, similarities, ctr, alpha=0.85, beta=0.7, convergence_threshold=0.0001):\n",
    "    N = len(links)\n",
    "    pr = np.ones(N) / N  # 初始均等分配PageRank\n",
    "    change = 1\n",
    "    while change > convergence_threshold:\n",
    "        new_pr = np.zeros(N)\n",
    "        for i in range(N):\n",
    "            link_contributions = 0\n",
    "            for j in links[i]:  # 遍历节点i的所有出链节点j\n",
    "                if len(links[j]) > 0:  # 避免除以零\n",
    "                    link_contributions += pr[j] * similarities[i][j] / len(links[j])\n",
    "            new_pr[i] = (1 - alpha) / N + alpha * (beta * link_contributions + (1 - beta) * ctr[i])\n",
    "        # 归一化新的PageRank值，确保它们的总和为1\n",
    "        new_pr /= np.sum(new_pr)  # 归一化步骤\n",
    "        \n",
    "        change = np.linalg.norm(new_pr - pr)\n",
    "        pr = new_pr\n",
    "    return pr\n"
   ],
   "id": "3e9007ac09d49c08",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T02:55:31.239922Z",
     "start_time": "2024-07-31T02:55:31.235236Z"
    }
   },
   "cell_type": "code",
   "source": "pr_with_ctr = page_rank_with_ctr(links, similarities, ctr)",
   "id": "8c06ec88bf92f1f0",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T02:55:32.045517Z",
     "start_time": "2024-07-31T02:55:32.030513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 按PageRank值对文档进行排序\n",
    "pagerank_score = pr_with_ctr\n",
    "doc_pagerank = list(zip(document_names, pagerank_score))\n",
    "\n",
    "# 按PageRank分数降序排序\n",
    "sorted_doc_pagerank = sorted(doc_pagerank, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 打印排序后的结果\n",
    "for doc_name, pr_score in sorted_doc_pagerank:\n",
    "    print(f\"{doc_name}: {pr_score}\")"
   ],
   "id": "4e1c8bf3171cb45c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10_技术在交通领域的变革.docx: 0.08001758953702781\n",
      "05_技术进步对经济的推动.docx: 0.07955876089571845\n",
      "15_技术在建筑行业的应用.docx: 0.0786462121886433\n",
      "20_技术在全球治理中的作用.docx: 0.07771522271598984\n",
      "19_技术与环境的可持续发展.docx: 0.0658173710786655\n",
      "09_智能技术的未来.docx: 0.06495895339669519\n",
      "14_技术在金融服务中的应用.docx: 0.0643529890581417\n",
      "04_技术在医疗中的应用.docx: 0.062285011410169504\n",
      "03_技术与教育.docx: 0.05109436418426403\n",
      "08_技术在农业中的应用.docx: 0.050777050122811855\n",
      "13_数字技术与数据安全.docx: 0.05039450388727602\n",
      "18_未来技术的道德挑战.docx: 0.048297693445973185\n",
      "12_技术推动的社会变化.docx: 0.03616673477649371\n",
      "02_现代技术的发展.docx: 0.035528903335643014\n",
      "07_技术与环境保护.docx: 0.03529284166733676\n",
      "17_技术与文化的互动.docx: 0.03480940285410139\n",
      "06_技术和社会变革.docx: 0.022021507697512224\n",
      "11_技术在航空业的革新.docx: 0.021045707157936307\n",
      "01_技术革命.docx: 0.0207020902725302\n",
      "16_技术对教育的长远影响.docx: 0.020517090317070028\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "12bd75142fac9ca3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
